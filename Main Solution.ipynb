{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59fad4e-a814-46a5-8dd2-db39eed8de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from datasets import Dataset as _Dataset, DatasetDict\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, classification_report\n",
    "\n",
    "import time\n",
    "import math\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import gc\n",
    "import re\n",
    "\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9222dbee-d010-48a6-94ac-00016a20aca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b85733-afd3-443b-94ac-54f6f03b7cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80173b53-9432-4ca0-9a59-70d2a37258a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4080 SUPER. Max memory: 15.992 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    #model_name = \"./models/llama_pretrain_ruen\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6432bb96-b2c6-40f3-93d3-63d6ef188bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2024.4 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.1, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dae3c33f-0615-4b5c-b0c4-9d400aeb5cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a29b20-e148-4e05-8ba5-9f94316f67d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eaffe1-6c08-4b01-8566-e99865287da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2f2027-a020-45ed-9f28-08fb2fba58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.read_csv('../data/sample_submission_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b7ff40-4676-4dc2-ad17-0c0bf0017d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_instruction = \"\"\"\n",
    "Transform statements to remove unnecessary or vulgar expressions while preserving the original meaning. Ensure clarity and politeness.\n",
    "Write only neutral sentence in {}.\n",
    "Answer in format => *neutral sentence*.\n",
    "<|eot_id|>\n",
    "\n",
    "Examples:\n",
    "\n",
    "he had steel balls too ! => he was brave too!\n",
    "\n",
    "have good advisers => i dont believe the saudis do not have good advisers\n",
    "\n",
    "solo para ser reemplazado por otro consenso sin cerebro => solo para ser reemplazado por otro consenso\n",
    "\n",
    "Weil er eine linke Bazille ist....ðŸ˜ƒ => Weil er ein Linker ist....ðŸ˜ƒ\n",
    "\n",
    "Ð´Ð°Ð»ÑŒÐ½ÐµÐ²Ð¾ÑÑ‚Ð¾Ñ‡Ð½Ñ‹Ðµ Ð°Ñ€Ð¼Ð¸Ñ Ñ€Ð°Ð·Ð³Ñ€Ð°Ð¼Ð¸Ð»Ð° Ñ„Ð°ÑˆÐ¸ÑÑ‚Ð¾Ð² Ð¿ÑƒÑÑ‚ÑŒ Ð¿Ð¾Ð¼Ð½ÑÑ‚ ÑÑƒÐºÐ¸ Ð¿ÑƒÑ‚Ð¸Ð½ÑÐºÐ¸Ðµ => Ð´Ð°Ð»ÑŒÐ½ÐµÐ²Ð¾ÑÑ‚Ð¾Ñ‡Ð½Ñ‹Ðµ Ð°Ñ€Ð¼Ð¸Ñ Ñ€Ð°Ð·Ð³Ñ€Ð°Ð¼Ð¸Ð»Ð° Ñ„Ð°ÑˆÐ¸ÑÑ‚Ð¾Ð² Ð¿ÑƒÑÑ‚ÑŒ Ð¿Ð¾Ð¼Ð½ÑÑ‚\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "input_template = \"\"\"\n",
    "{}\"\"\"\n",
    "\n",
    "lang_map = {\n",
    "    'uk': 'ukrainian',\n",
    "    'en': 'english',\n",
    "    'ru': 'russian',\n",
    "    'hi': 'hindi',\n",
    "    'de': 'german',\n",
    "    'es': 'spanish',\n",
    "    'am': 'amharic',\n",
    "    'zh': 'chinese',\n",
    "    'ar': 'arabic'\n",
    "}\n",
    "\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def prepare_input_pred(df):\n",
    "    return alpaca_prompt.format(\n",
    "        '' +\\\n",
    "        expert_instruction.format(lang_map[df['lang']]),\n",
    "        input_template.format(\n",
    "            df['toxic_sentence']\n",
    "        ),\n",
    "        '=>'\n",
    "    )\n",
    "\n",
    "submit_df['text'] = submit_df.apply(prepare_input_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9362cab3-954e-4614-b2ae-b1964e23a36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "301eb84c-c766-470c-b7a9-2c3c761cca5a",
   "metadata": {},
   "source": [
    "### Work example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcfc91ff-01ac-46bd-8830-aada9efeecee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "\n",
      "Transform statements to remove unnecessary or vulgar expressions while preserving the original meaning. Ensure clarity and politeness.\n",
      "Write only neutral sentence in ukrainian.\n",
      "Answer in format => *neutral sentence*.\n",
      "<|eot_id|>\n",
      "\n",
      "Examples:\n",
      "\n",
      "he had steel balls too! => he was brave too!\n",
      "\n",
      "have good advisers => i dont believe the saudis do not have good advisers\n",
      "\n",
      "solo para ser reemplazado por otro consenso sin cerebro => solo para ser reemplazado por otro consenso\n",
      "\n",
      "Weil er eine linke Bazille ist....ðŸ˜ƒ => Weil er ein Linker ist....ðŸ˜ƒ\n",
      "\n",
      "Ð´Ð°Ð»ÑŒÐ½ÐµÐ²Ð¾ÑÑ‚Ð¾Ñ‡Ð½Ñ‹Ðµ Ð°Ñ€Ð¼Ð¸Ñ Ñ€Ð°Ð·Ð³Ñ€Ð°Ð¼Ð¸Ð»Ð° Ñ„Ð°ÑˆÐ¸ÑÑ‚Ð¾Ð² Ð¿ÑƒÑÑ‚ÑŒ Ð¿Ð¾Ð¼Ð½ÑÑ‚ ÑÑƒÐºÐ¸ Ð¿ÑƒÑ‚Ð¸Ð½ÑÐºÐ¸Ðµ => Ð´Ð°Ð»ÑŒÐ½ÐµÐ²Ð¾ÑÑ‚Ð¾Ñ‡Ð½Ñ‹Ðµ Ð°Ñ€Ð¼Ð¸Ñ Ñ€Ð°Ð·Ð³Ñ€Ð°Ð¼Ð¸Ð»Ð° Ñ„Ð°ÑˆÐ¸ÑÑ‚Ð¾Ð² Ð¿ÑƒÑÑ‚ÑŒ Ð¿Ð¾Ð¼Ð½ÑÑ‚\n",
      " \n",
      "\n",
      "\n",
      "### Input:\n",
      "\n",
      "Ð† Ñ€Ð¸Ð±ÐºÑƒ, Ñ– Ð½Ð° Ñ…ÑƒÐ¹, Ñ‚Ð°?\n",
      "\n",
      "### Response:\n",
      "=> Ð† Ñ€Ð¸Ð±ÐºÑƒ, Ñ– Ð½Ð° Ð¼Ð¾Ñ€Ðµ, Ñ‚Ð°?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    submit_df['text'][222]\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, do_sample=True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453e6a3-ceb3-4fa7-b501-082c737bbbfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed1b34df-ccab-4b03-ab04-af8c1cb147c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_answers2 = []\n",
    "\n",
    "def rem(inp, out):\n",
    "    bot = '<|begin_of_text|>'\n",
    "    eot = '<|end_of_text|>'\n",
    "    res = out.replace(bot, '').split('### Response:\\n')[1]\n",
    "    try:\n",
    "        res = res.split('=>')[1].split('<|eot_id|>')[0]\n",
    "    except Exception as e:\n",
    "        print(res)\n",
    "\n",
    "    return res\n",
    "\n",
    "def run_batch_gen_new(input_texts, BS = 64):\n",
    "    tokenizer.pad_token = \"<|end_of_text|>\"\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    global decoded_answers2\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    for batch_start in (tqdm(range(0, len(input_texts), BS))):\n",
    "        input_texts_i = input_texts[batch_start:batch_start+BS]\n",
    "        inputs = tokenizer(input_texts_i, return_tensors = \"pt\", padding = True).to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens = 150, use_cache = True, early_stopping=True, do_sample = True)\n",
    "        decoded = tokenizer.batch_decode(outputs)\n",
    "\n",
    "        decoded = [rem(input_texts_i[j], d) for j, d in enumerate(decoded)]\n",
    "        \n",
    "        decoded_answers2 += decoded\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        #break\n",
    "\n",
    "    return decoded_answers2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b6801-2c0c-4203-9276-5574e49e27af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "sumit_result = run_batch_gen_new(submit_df['text'].tolist(), BS=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb6c43-5f70-4cd5-a3ec-a323b35976ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmb_sum = [s.split('(Note:')[0] if '(Note:' in s else s for s in cmb_sum]\n",
    "cmb_sum = [s.split('\\n')[0] if '\\n' in s else s for s in cmb_sum]\n",
    "cmb_sum = [s.replace('<|end_of_text|>', '').replace('<|eot_id|>', '') for s in cmb_sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b317e35-4d89-49de-8c4e-20e6fed5171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df['neutral_sentence'] = cmb_sum\n",
    "submit_df[['toxic_sentence', 'neutral_sentence', 'lang']].to_csv('./sumbissions/llama_prompt_4.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
